import os
from dotenv import load_dotenv
from groq import Groq

load_dotenv()
API_KEY = os.getenv("GROQ_API_KEY")
class FinalGenerator:
    def __init__(self, api_key=API_KEY, model="meta-llama/llama-4-maverick-17b-128e-instruct"):
        if not api_key:
            raise ValueError("La cl√© API Groq n'est pas d√©finie. V√©rifie ton fichier .env.")
        self.client = Groq(api_key=api_key)
        self.model = model
    def generate(self,question: str,intent: str,clarification: str | None ,rag_snippets: list[str],memory: dict,user_profile: str) -> str:
        """
        G√©n√®re la r√©ponse finale du chatbot.

        :param question: question utilisateur (clarifi√©e si applicable)
        :param intent: intention d√©tect√©e
        :param clarification: texte de clarification (si pr√©sent)
        :param rag_snippets: extraits RAG optionnels
        :param memory: dictionnaire d'informations m√©dicales connues
        :param user_profile: ex. "Patient" ou "M√©decin"
        :return: r√©ponse g√©n√©r√©e
        """

        
        system_prompt = (
            "Tu es un assistant m√©dical intelligent multilingue. "
            "Tu fournis des r√©ponses claires, rigoureuses et adapt√©es au profil : "
            f"{user_profile.lower()}.\n"
            "R√©ponds toujours dans la langue utilis√©e par l'utilisateur.\n"
            "Si la question rel√®ve d‚Äôun domaine critique (urgence, cancer...), reste neutre et sugg√®re de consulter un professionnel."
        )

        context_parts = [f"üß† Intention d√©tect√©e : {intent}"]
        if clarification:
            context_parts.append(f"‚ùì Clarification : {clarification}")
        if memory:
            memory_str = "\n".join(f"{k}: {v}" for k, v in memory.items())
            context_parts.append(f"üìÇ Dossier connu :\n{memory_str}")
        if rag_snippets:
            context_parts.append("üìö Informations scientifiques :\n" + "\n---\n".join(rag_snippets))

        context_parts.append(f"üìù Question : {question}")
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": "\n\n".join(context_parts)}
        ]

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=800
        )

        return response.choices[0].message.content.strip()

if __name__ == "__main__":
    generator=FinalGenerator()
    print(generator.generate())